# Deeplearning_Framework
This project contain two project files first code demonstration is placed inside rubric 1 file which is project that predicts streamflow using past weather and environmental data with an LSTM model. The dataset is preprocessed by encoding dates numerically and normalizing features with MinMaxScaler. Time-series sequences of 30 steps are created, where each sequence predicts the streamflow of the next step. The data is split into training and testing sets, and an LSTM model with two layers (64 and 32 units) is trained over 100 epochs using the Adam optimizer and MSE loss. After training, predictions are evaluated using Mean Squared Error and visualized alongside actual values to assess the model's performance in capturing streamflow patterns. This approach effectively models temporal dependencies for accurate streamflow forecasting.

The second code demonstration is placed inside rubric 2 files. This project is slight modified version of the project demonstrated in rubric 1. It combines a LSTM layer and a fully connected (FC) layer.
Fine-tuning the code involves tweaking key parameters to enhance the model's performance. For example, adjusting the hidden dimension (hidden_dim) can help the model better capture patterns in the data, while working with pretrained weights allows you to freeze or unfreeze layers depending on the task. Training aspects like learning rate, batch size, and the number of epochs can be modified to find the right balance between learning speed and accuracy, with tools like learning rate schedulers helping fine-tune this process further. Adding regularization techniques, such as weight decay, helps avoid overfitting, while trying out different optimizers like SGD with momentum or loss functions like MAE or SmoothL1Loss can improve how the model handles errors. You can also experiment with input sequence lengths and focus on selecting the most important features (input_dim) to ensure the LSTM is learning effectively. The key is to gradually refine these parameters while keeping an eye on the loss and model predictions to achieve the best results.

**Note: The datasets used in both of these project is real dataset collected from different checkpoint of streamflow around in Nepal. There were different checkpoints and data was too large I only used data collected from 240 checkpoint.**
